{"id": "2511.10706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10706", "abs": "https://arxiv.org/abs/2511.10706", "authors": ["Zitong Zhang", "Hao Sun"], "title": "Differentiable Sparse Identification of Lagrangian Dynamics", "comment": null, "summary": "Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u5fae\u5206\u7a00\u758f\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u6b21B\u6837\u6761\u8fd1\u4f3c\u3001\u9c81\u68d2\u65b9\u7a0b\u53d1\u73b0\u673a\u5236\u548c\u9012\u5f52\u5bfc\u6570\u8ba1\u7b97\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u8bc6\u522b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u548c\u6570\u636e\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u62c9\u683c\u6717\u65e5\u8bc6\u522b\u65b9\u6cd5\u53d7\u5230\u6d4b\u91cf\u566a\u58f0\u548c\u6709\u9650\u6570\u636e\u7684\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u7cfb\u7edf\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u4e09\u6b21B\u6837\u6761\u8fd1\u4f3c\u5230\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u8bc6\u522b\u4e2d\uff0c\u91c7\u7528\u9c81\u68d2\u65b9\u7a0b\u53d1\u73b0\u673a\u5236\u7ed3\u5408\u7269\u7406\u7ea6\u675f\uff0c\u4ee5\u53ca\u57fa\u4e8eB\u6837\u6761\u57fa\u51fd\u6570\u7684\u9012\u5f52\u5bfc\u6570\u8ba1\u7b97\u65b9\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u673a\u68b0\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u53ef\u9760\u5730\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u63d0\u53d6\u7269\u7406\u89c4\u5f8b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u8bc6\u522b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u548c\u6570\u636e\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7684\u6570\u636e\u9a71\u52a8\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11533", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11533", "abs": "https://arxiv.org/abs/2511.11533", "authors": ["Jueun Kwon", "Max M. Sun", "Todd Murphey"], "title": "Volumetric Ergodic Control", "comment": "8 pages, 8 figures", "summary": "Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f53\u79ef\u72b6\u6001\u8868\u793a\u7684\u904d\u5386\u63a7\u5236\u65b9\u6cd5\uff0c\u4f18\u5316\u7a7a\u95f4\u8986\u76d6\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5c06\u8986\u76d6\u7387\u63d0\u9ad8\u4e86\u4e24\u500d\u4ee5\u4e0a\uff0c\u5e76\u5728\u673a\u68b0\u64e6\u9664\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u904d\u5386\u63a7\u5236\u65b9\u6cd5\u5c06\u673a\u5668\u4eba\u5efa\u6a21\u4e3a\u975e\u4f53\u79ef\u70b9\uff0c\u4f46\u5b9e\u9645\u4e2d\u673a\u5668\u4eba\u901a\u8fc7\u5177\u6709\u7269\u7406\u4f53\u79ef\u7684\u8eab\u4f53\u548c\u4f20\u611f\u5668\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u4f53\u79ef\u8868\u793a\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4f53\u79ef\u72b6\u6001\u8868\u793a\u7684\u65b0\u904d\u5386\u63a7\u5236\u516c\u5f0f\uff0c\u652f\u6301\u4efb\u610f\u57fa\u4e8e\u91c7\u6837\u7684\u4f53\u79ef\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u904d\u5386\u63a7\u5236\u6e10\u8fd1\u8986\u76d6\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u4e3a\u5b9e\u65f6\u63a7\u5236\u6dfb\u52a0\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u641c\u7d22\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u4f7f\u7528\u591a\u79cd\u673a\u5668\u4eba\u52a8\u529b\u5b66\u548c\u672b\u7aef\u6267\u884c\u5668\u51e0\u4f55\u5f62\u72b6\u6216\u4f20\u611f\u5668\u6a21\u578b\uff0c\u8986\u76d6\u7387\u6548\u7387\u63d0\u9ad8\u4e86\u4e24\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301100%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u673a\u68b0\u64e6\u9664\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u6807\u51c6\u904d\u5386\u63a7\u5236\u65b9\u6cd5\uff0c\u4e3a\u4f53\u79ef\u611f\u77e5\u7684\u7a7a\u95f4\u8986\u76d6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11552", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11552", "abs": "https://arxiv.org/abs/2511.11552", "authors": ["Dawei Zhu", "Rui Meng", "Jiefeng Chen", "Sujian Li", "Tomas Pfister", "Jinsung Yoon"], "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding", "comment": null, "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.", "AI": {"tldr": "DocLens\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7c7b\u4f3c\u955c\u5934\u7684\u673a\u5236\u6709\u6548\u5b9a\u4f4d\u8bc1\u636e\uff0c\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u7279\u5b9a\u9875\u9762\u4e0a\u7684\u89c6\u89c9\u5143\u7d20\uff0c\u5e76\u4f7f\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u53ef\u9760\u7b54\u6848\uff0c\u5728\u957f\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u89c9\u6587\u6863\u65f6\u9762\u4e34\u8bc1\u636e\u5b9a\u4f4d\u7684\u6839\u672c\u6311\u6218\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u7d22\u76f8\u5173\u9875\u9762\u5e76\u5ffd\u7565\u89c6\u89c9\u5143\u7d20\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u6027\u80fd\u6709\u9650\u548c\u6a21\u578b\u5e7b\u89c9\u3002", "method": "\u63d0\u51faDocLens\u6846\u67b6\uff1a1\uff09\u4ece\u5b8c\u6574\u6587\u6863\u5bfc\u822a\u5230\u76f8\u5173\u9875\u9762\u4e0a\u7684\u7279\u5b9a\u89c6\u89c9\u5143\u7d20\uff1b2\uff09\u91c7\u7528\u91c7\u6837-\u88c1\u51b3\u673a\u5236\u751f\u6210\u5355\u4e00\u53ef\u9760\u7b54\u6848\uff1b3\uff09\u4e0eGemini-2.5-Pro\u914d\u5bf9\u4f7f\u7528\u3002", "result": "\u5728MMLongBench-Doc\u548cFinRAGBench-V\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4e2d\u5fc3\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DocLens\u901a\u8fc7\u589e\u5f3a\u7684\u5b9a\u4f4d\u80fd\u529b\u5c55\u793a\u4e86\u5176\u5728\u957f\u89c6\u89c9\u6587\u6863\u7406\u89e3\u4e2d\u7684\u5f3a\u5927\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u8bc1\u636e\u5b9a\u4f4d\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
