<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN是一个专注于提升视频生成运动质量的训练后框架，通过光学流判别器和分布匹配正则化器，在不牺牲视觉保真度的情况下显著改善运动真实感。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在帧级保真度上表现良好，但在运动连贯性、动态性和真实感方面存在不足，容易出现抖动、重影或不合理的动态效果。标准的去噪MSE目标缺乏对时间一致性的直接监督。

Method: 基于3步蒸馏的视频扩散模型，训练基于DiT的光学流判别器来区分真实与生成的运动，并结合分布匹配正则化器来保持视觉保真度。

Result: 在VBench上，MoGAN比50步教师模型提升运动得分7.3%，比3步DMD模型提升13.3%；在VideoJAM-Bench上分别提升7.4%和8.8%，同时保持相当或更好的美学和图像质量得分。人类研究也证实MoGAN在运动质量上更受青睐。

Conclusion: MoGAN在不牺牲视觉保真度或效率的情况下，显著提升了运动真实感，为快速高质量视频生成提供了实用路径。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [2] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 提出了一种自提示、点监督的框架，仅使用稀疏点标注将SAM适配到遥感图像，通过Refine-Requery-Reinforce循环提升分割质量。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在遥感图像上由于领域偏移和密集标注稀缺导致的性能下降问题。

Method: 采用Refine-Requery-Reinforce循环：从初始点生成粗伪掩码(Refine)，用自构建框提示改进(Requery)，通过嵌入对齐减少确认偏差(Reinforce)。

Result: 在WHU、HRSID和NWPU VHR-10三个遥感基准数据集上，方法持续超越预训练SAM和最近的点监督分割方法。

Conclusion: 自提示和语义对齐为遥感应用中基础分割模型的可扩展点级适配提供了高效路径。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [3] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL是Qwen系列中最强大的视觉语言模型，在多种多模态基准测试中表现优异，支持高达256K tokens的交错上下文，无缝集成文本、图像和视频。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理交错多模态输入（文本、图像、视频）的先进视觉语言模型，满足现实工作流程中对图像推理、智能决策和多模态代码智能的需求。

Method: 采用增强的交错MRoPE进行时空建模，集成DeepStack以利用多级ViT特征加强视觉-语言对齐，以及基于文本的时间对齐技术来改进视频处理。

Result: 在MMMU、MathVista和MathVision等综合评估中表现出领先性能，在纯文本理解、长上下文理解和多模态推理方面均优于同类模型。

Conclusion: Qwen3-VL作为一个基础引擎，在图像推理、智能决策和多模态代码智能方面具有广泛应用前景，在密集和MoE架构下均实现了卓越性能。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [4] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出了一种新颖的AI错误校正系统，通过少样本学习在资源受限设备上实现高效错误修正，结合服务器端基础模型训练和设备端原型分类，无需模型重训练即可更新原型。


<details>
  <summary>Details</summary>
Motivation: AI模型在日常设备中的普及面临预测错误问题，现有解决方案主要关注错误检测而缺乏高效修正机制，特别是在资源受限设备上。

Method: 系统包含两个关键组件：(1) 服务器端管道利用知识蒸馏将基础模型的鲁棒特征表示转移到设备兼容架构；(2) 设备端机制通过原型适配实现超高效错误修正。

Result: 在Food-101和Flowers-102数据集上，单次场景下实现超过50%的错误修正率，遗忘率低于0.02%，计算开销可忽略不计。

Conclusion: 该系统通过Android演示应用验证了在实际场景中的实用性，为资源受限设备上的AI错误修正提供了可行解决方案。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [5] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: CaFlow是一个用于长时动作质量评估的统一框架，结合了反事实去混淆和双向时间条件流，通过自监督方式解耦因果特征和混淆特征，并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 长时动作质量评估（如花样滑冰或艺术体操）面临建模长时间动态和保持对上下文混淆因素鲁棒性的挑战。现有方法依赖昂贵标注或单向时序建模，容易受到伪相关性和不稳定长期表示的影响。

Method: 提出CaFlow框架，包含因果反事实正则化（CCR）模块和BiT-Flow模块。CCR模块自监督解耦因果和混淆特征，通过反事实干预增强因果鲁棒性；BiT-Flow模块通过循环一致性约束建模前向和后向动态，产生更平滑连贯的表示。

Result: 在多个长时AQA基准测试上的广泛实验表明，CaFlow实现了最先进的性能。

Conclusion: CaFlow通过整合反事实去混淆和双向时间条件流，有效解决了长时动作质量评估中的时序建模和鲁棒性问题，为相关应用提供了有力工具。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [6] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: Multi-Crit是一个评估多模态模型作为评判者能力的基准，重点关注模型遵循多样化细粒度评估标准的能力，涵盖开放式生成和可验证推理任务。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型被广泛用作多模态评估系统的评判者，但它们遵循多样化、细粒度评估标准的能力尚未得到充分探索。

Method: 通过严格的数据整理流程构建Multi-Crit基准，收集具有多标准人工标注的挑战性响应对，并引入三个新指标系统评估多元标准遵循、标准切换灵活性和识别标准级别偏好冲突的能力。

Result: 对25个LMM的综合分析显示：1)专有模型在遵循多元标准方面仍存在困难，特别是在开放式评估中；2)开源模型在灵活遵循多样化标准方面进一步落后；3)使用整体判断信号进行批评微调能增强视觉基础，但无法推广到多元标准级判断。

Conclusion: Multi-Crit为构建可靠且可操控的多模态AI评估奠定了基础，揭示了当前多模态评判者的局限性。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [7] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: 提出了ADVLA框架，通过在视觉编码器投影到文本特征空间的特征上直接应用对抗扰动，有效破坏VLA模型的下游动作预测，具有低幅度、局部稀疏的特点，避免了传统补丁攻击的高训练成本和明显扰动。


<details>
  <summary>Details</summary>
Motivation: 解决现有对抗攻击方法需要昂贵的端到端训练且通常生成明显扰动补丁的问题，开发一种更高效、更隐蔽的攻击框架。

Method: 直接在视觉编码器投影到文本特征空间的特征上应用对抗扰动，采用注意力引导使扰动既集中又稀疏，引入三种策略增强敏感性、强制稀疏性和集中扰动。

Result: 在L∞=4/255约束下，ADVLA结合Top-K掩码修改不到10%的补丁，攻击成功率接近100%，扰动集中在关键区域，整体图像几乎不可察觉，单步迭代仅需约0.06秒。

Conclusion: ADVLA在低幅度和局部稀疏条件下有效削弱VLA模型的下游动作预测，避免了传统补丁攻击的高训练成本和明显扰动，展示了攻击VLA特征空间的独特有效性和实用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [8] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 本文首次系统研究仅通过相机轨迹（而非像素）来感知视频内容的可行性，提出了CamFormer对比学习框架，证明相机轨迹是揭示视频内容的信息丰富信号。


<details>
  <summary>Details</summary>
Motivation: 探索是否能够不依赖像素信息，仅通过相机运动轨迹来理解视频内容，这是一个看似不可能但具有重要价值的研究问题。

Method: 提出对比学习框架训练CamFormer编码器，将相机姿态轨迹投影到联合嵌入空间，并与自然语言对齐。

Result: 发现相机轨迹是揭示视频内容的有效信号，CamFormer表示在跨模态对齐、分类和时间分析等任务中表现出色，且对不同的相机姿态估计方法具有鲁棒性。

Conclusion: 相机轨迹是一种轻量级、鲁棒且多功能的视频内容感知模态，"如何移动"确实可以揭示"在做什么"或"在观察什么"。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [9] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: G²VLM是一个几何基础的视觉语言模型，通过整合3D视觉几何特征来提升空间智能，包括空间3D重建和空间理解任务。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间智能方面存在不足，缺乏从2D图像重建3D空间的视觉几何学习过程。

Method: 提出G²VLM模型，利用学习的3D视觉几何特征直接预测3D属性，并通过上下文学习和交错推理增强空间推理任务。该统一设计可扩展训练多视角图像和视频数据。

Result: G²VLM在3D重建任务上达到最先进水平，在空间理解和推理任务上取得更好或竞争性结果。

Conclusion: 通过将语义强大的VLM与低级3D视觉任务统一，G²VLM可作为社区的强基线，并解锁更多未来应用如3D场景编辑。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [10] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: Canvas-to-Image是一个统一的框架，将文本提示、主题参考、空间排列、姿态约束和布局注释等多种控制信号整合到单一画布界面中，通过多任务画布训练策略，使扩散模型能够联合理解和集成异构控制，显著提升了图像生成的身份保持和控制遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现代扩散模型在生成高质量和多样化图像方面表现出色，但在高保真度的组合和多模态控制方面仍存在困难，特别是当用户同时指定文本提示、主题参考、空间排列、姿态约束和布局注释时。

Method: 提出Canvas-to-Image框架，将多样控制信号编码到单一复合画布图像中，使模型能够直接解释以进行集成的视觉空间推理。策划多任务数据集并提出多任务画布训练策略，在统一学习范式下优化扩散模型联合理解和集成异构控制。

Result: 在包括多人组合、姿态控制组合、布局约束生成和多控制生成等具有挑战性的基准测试中，Canvas-to-Image在身份保持和控制遵循方面显著优于最先进的方法。

Conclusion: Canvas-to-Image通过统一框架和多任务训练策略，实现了对多种控制模态的跨模态推理，在推理时能够很好地泛化到多控制场景，显著提升了图像生成的控制精度和保真度。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [11] [Uncertainty Quantification for Visual Object Pose Estimation](https://arxiv.org/abs/2511.21666)
*Lorenzo Shaikewitz,Charis Georgiou,Luca Carlone*

Main category: cs.RO

TL;DR: SLUE是一种用于单目姿态估计的分布自由不确定性边界方法，通过凸优化生成包含真实物体姿态的高概率椭球不确定性边界，无需初始猜测或严格分布假设。


<details>
  <summary>Details</summary>
Motivation: 在机器人技术中，量化物体姿态估计的不确定性对于鲁棒控制和规划至关重要，但现有方法通常需要严格的分布假设，缺乏统计上严格的不确定性量化方法。

Method: 基于2D语义关键点检测的高概率噪声边界，SLUE使用S-引理启发的最小体积包围椭球问题松弛，通过凸优化生成椭球不确定性边界，并扩展到平方和松弛层次以获得更紧密的边界。

Result: 在两个姿态估计数据集和真实无人机跟踪场景中，SLUE相比现有方法生成显著更小的平移边界和具有竞争力的方向边界。

Conclusion: SLUE提供了一种统计上严格的分布自由姿态不确定性量化方法，能够生成紧凑且可靠的不确定性边界，适用于实际机器人应用。

Abstract: Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.

</details>


### [12] [TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/abs/2511.21690)
*Seungjae Lee,Yoonkyo Jung,Inkook Chun,Yao-Chih Lee,Zikui Cai,Hongjia Huang,Aayush Talreja,Tan Dat Dao,Yongyuan Liang,Jia-Bin Huang,Furong Huang*

Main category: cs.RO

TL;DR: TraceGen是一个世界模型，通过符号化的3D轨迹空间表示，能够从少量演示中学习机器人任务，实现跨平台、跨环境和跨任务的迁移学习。


<details>
  <summary>Details</summary>
Motivation: 解决机器人从少量演示中学习新任务的挑战，利用丰富的跨平台视频资源（人类和不同机器人），克服平台差异、相机视角和环境变化带来的障碍。

Method: 提出统一的符号化表示——3D轨迹空间，开发TraceGen世界模型预测轨迹空间中的运动，并构建TraceForge数据管道将异构视频转换为一致的3D轨迹数据。

Result: 在仅使用5个目标机器人视频的情况下，TraceGen在四个任务上达到80%成功率，推理速度比现有视频世界模型快50-600倍；仅用5个手机拍摄的人类演示视频，在真实机器人上仍能达到67.5%成功率。

Conclusion: TraceGen通过3D轨迹空间表示有效解决了小数据学习问题，实现了跨平台的快速适应，无需依赖物体检测器或繁重的像素空间生成。

Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 本文通过降维方法提取、处理和可视化基于Transformer的语言模型的潜在状态几何结构，揭示了注意力机制和MLP组件在中间层的明显分离模式，以及位置嵌入的高维螺旋结构等几何特征。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在许多自然语言任务中取得了最先进的结果，但其内部机制仍然难以解释。本文旨在通过系统分析Transformer内部结构来支持可复现的可解释性研究。

Method: 使用主成分分析（PCA）和均匀流形逼近（UMAP）等降维技术，在Transformer块中的多个点捕获层间激活，并对GPT-2和LLaMa模型进行实验。

Result: 发现了中间层注意力机制和MLP组件输出的明显分离模式，识别了初始序列位置潜在状态的高范数特征，可视化了潜在状态的层间演化，以及GPT-2位置嵌入的高维螺旋结构和LLaMa的序列级几何模式。

Conclusion: 该方法支持对Transformer内部结构的系统分析，为可复现的可解释性研究提供了工具和洞察，代码已开源供进一步研究使用。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [14] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 论文通过实验发现，2012-2023年间AI训练效率提升的22,000倍中，只有不到100倍来自算法创新，大部分效率提升源于算法在规模扩大时的效率增益，特别是LSTM到Transformer的转变。


<details>
  <summary>Details</summary>
Motivation: 研究2012-2023年间AI训练效率提升22,000倍的真实原因，挑战传统认为算法创新是主要驱动力的假设。

Method: 进行小规模消融实验分析关键创新，进行扩展实验比较LSTM和Transformer在不同规模下的效率表现。

Result: 发现算法效率提升与计算规模密切相关，LSTM到Transformer的转变贡献了大部分效率增益，通过实验外推和文献估计解释了6,930倍效率提升。

Conclusion: 小模型算法进展远慢于预期，算法效率衡量具有强烈的参考依赖性，规模依赖的效率改进是主要驱动因素。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [15] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 研究发现深层视觉Transformer性能不如浅层，存在Cliff-Plateau-Climb三阶段模式，[CLS]令牌逐渐边缘化，信息扩散而非任务性能提升是深层模型主要特征。


<details>
  <summary>Details</summary>
Motivation: 解决深层视觉Transformer性能下降问题，探索表示随深度演化的规律。

Method: 对ViT-S、ViT-B和ViT-L进行系统实证分析，使用信息混洗指数量化信息混合模式。

Result: 发现[CLS]令牌逐渐边缘化，ViT-L的信息-任务权衡比ViT-B晚10层出现，深层模型更多是信息扩散而非性能提升。

Conclusion: Transformer架构应注重精心校准的深度执行清晰阶段转换，而非简单增加参数数量；信息混洗指数可作为有用诊断工具。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [16] [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)
*Daniel R. Jiang,Jalaj Bhandari,Yukai Yang,Rémi Munos,Tyler Lu*

Main category: cs.LG

TL;DR: 提出Iterative PPO方法，将多轮对话RL问题转化为单轮RLHF问题，通过交替拟合Q函数和改进策略来优化LLM在目标导向对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 优化LLM在目标导向多轮对话中的表现面临稀疏奖励和响应级规划与令牌级生成不匹配的挑战。

Method: 将多轮RL问题形式化地简化为一系列单轮RLHF问题，使用学习到的多轮Q函数作为单轮问题的奖励模型，提出Iterative PPO算法交替拟合Q函数和改进策略。

Result: 证明使用标准令牌级PPO解决单轮RL问题等价于多轮问题中的策略改进步骤，可直接利用稳定的单轮RLHF工具实现。

Conclusion: 该方法在完全在线和完全离线方法之间找到平衡点，既保持了在线更新的适应性，又获得了离线训练的稳定性优势。

Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

</details>


### [17] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 该论文质疑电信AI训练中所有样本同等重要的假设，提出基于样本重要性分析的框架，在保持性能的同时减少计算需求和能耗。


<details>
  <summary>Details</summary>
Motivation: 电信AI应用中数据量大、噪声多、标注成本高，但现有方法假设所有训练样本同等重要，这与下一代系统对准确、高效、可持续AI模型的需求不符。

Method: 通过跨epoch的样本级梯度分析识别样本影响力和冗余模式，提出选择性优先处理重要数据的样本重要性框架。

Result: 在三个真实世界电信数据集上的实验表明，该方法在保持性能的同时减少了数据需求和计算开销。

Conclusion: 该方法通过优化样本选择策略，推进了电信领域可持续AI的目标，在不牺牲准确性的前提下显著降低了计算和能源消耗。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 该研究测试了大型语言模型在8拼图任务中的规划和状态推理能力，发现即使有反馈和验证器辅助，模型仍存在内部状态表示脆弱和启发式规划能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在规划和状态推理方面的真实能力，特别是在需要状态跟踪和目标导向规划的任务中，而不依赖代码执行或其他工具。

Method: 使用8拼图任务测试四种模型，采用零样本、思维链和算法思维等提示方法，并分层次提供纠正反馈，最后使用外部移动验证器辅助。

Result: 反馈提高了某些模型-提示组合的成功率，但成功运行通常冗长且计算昂贵。即使有移动验证器辅助，所有模型都无法解决任何拼图。模型存在内部状态表示脆弱和启发式规划能力弱的问题。

Conclusion: 当前LLMs在没有外部工具的情况下，在规划方面存在显著局限性，需要开发维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [19] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 该论文将系统动力学和结构方程建模整合到一个共同的数学框架中，以解决AI/ML模型开发中的偏见问题，并促进负责任AI的发展。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型在解决未解决问题方面具有创新性，但会无意中放大人类偏见。负责任AI倡导者希望利用更丰富的系统动力学因果模型来指导开发，但不同方法基于不同假设的整合存在困难。

Method: 将系统动力学和结构方程建模整合到一个共同的数学框架中，用于从分布生成系统、开发方法，并比较结果。

Result: 建立了一个统一的数学框架，能够协调系统动力学和结构方程建模的不同假设基础。

Conclusion: 该框架可以为数据科学和AI/ML应用提供系统动力学的认识论基础，促进负责任AI的发展。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>
